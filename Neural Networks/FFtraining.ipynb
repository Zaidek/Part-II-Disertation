{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ML libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "with open(\"../data/training_data\", \"rb\") as fb:\n",
    "    training_data = pickle.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import testing data\n",
    "with open(\"../data/testing_data\", \"rb\") as fb:\n",
    "    testing_data = pickle.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try set gpu as training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "training_data = training_data.loc[training_data.type == \"story\"]\n",
    "testing_data = testing_data.loc[testing_data.type == \"story\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_indexed = training_data.reset_index(drop=True)\n",
    "testing_data_indexed = testing_data.reset_index(drop=True)\n",
    "print(training_data_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preproccess(text):\n",
    "    if isinstance(text, float):\n",
    "        return [\"\"]\n",
    "    tokens = re.split('\\s+', text)\n",
    "    tokens = [\"\".join([i for i in x if i not in string.punctuation]) for x in tokens]\n",
    "    tokens = [re.sub(\"\\d-\", \"\", x) for x in tokens]\n",
    "    tokens = [x.lower() for x in tokens]\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    final = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    if isinstance(final, float):\n",
    "        return [\"\"]\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_titles_processed = [preproccess(x) for x in training_data_indexed[\"title\"]]\n",
    "#training_text_processed = [preproccess(x) for x in training_data_indexed[\"text\"]]\n",
    "training_titles_processed = training_data_indexed[\"title\"].apply(lambda x: preproccess(x))\n",
    "training_text_processed = training_data_indexed[\"text\"].apply(lambda x: preproccess(x))\n",
    "testing_titles_processed = testing_data_indexed[\"title\"].apply(lambda x: preproccess(x))\n",
    "testing_text_processed = testing_data_indexed[\"text\"].apply(lambda x: preproccess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titles_vocab_train = list(dict.fromkeys(training_titles_processed.apply(pd.Series).stack().reset_index(drop = True)))\n",
    "#titles_vocab_test = list(dict.fromkeys(testing_titles_processed.apply(pd.Series).stack().reset_index(drop = True)))\n",
    "#titles_vocab = list(dict.fromkeys(titles_vocab_train + titles_vocab_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_vocab_train = list(dict.fromkeys(training_text_processed.apply(pd.Series).stack().reset_index(drop = True)))\n",
    "#text_vocab_test = list(dict.fromkeys(testing_text_processed.apply(pd.Series).stack().reset_index(drop = True)))\n",
    "#text_vocab = list(dict.fromkeys(text_vocab_train + text_vocab_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"titles_vocab\", \"wb\") as fb:\n",
    "#    pickle.dump(titles_vocab, fb)\n",
    "\n",
    "#with open(\"text_vocab\", \"wb\") as fb:\n",
    "#    pickle.dump(text_vocab, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"titles_vocab\", \"rb\") as fb:\n",
    "    titles_vocab = pickle.load(fb)\n",
    "\n",
    "with open(\"text_vocab\", \"rb\") as fb:\n",
    "    text_vocab = pickle.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a special empty token for posts with empty titles or text\n",
    "titles_vocab.append(\"<|empty|>\")\n",
    "text_vocab.append(\"<|empty|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data_indexed.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_cols_with_url = [\"title\", \"url\", \"text\", \"time\"]\n",
    "req_cols_without_url = [\"title\", \"text\", \"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = training_data_indexed.score\n",
    "training_data_indexed = training_data_indexed[req_cols_without_url]\n",
    "\n",
    "testing_scores = testing_data_indexed.score\n",
    "testing_data_indexed = testing_data_indexed[req_cols_without_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data_indexed.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW_bin(words, vocab):\n",
    "    return [1 if word in words else 0 for word in vocab]\n",
    "\n",
    "def BOW_freq(words, vocab):\n",
    "    return [words.count(word) for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(url):\n",
    "    if not isinstance(url, str):\n",
    "        return \"\"\n",
    "    return urlparse(url).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cutoff pont for post being p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformations of the data\n",
    "\n",
    "class TextualTransform1(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "        \n",
    "        post[\"title\"] = BOW_bin(preproccess(post[\"title\"]), titles_vocab)\n",
    "        post[\"text\"] = BOW_bin(preproccess(post[\"text\"]), text_vocab)\n",
    "\n",
    "        return {'post': post, 'score': score}\n",
    "\n",
    "class TextualTransform2(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "        \n",
    "        post[\"title\"] = BOW_freq(preproccess(post[\"title\"]), titles_vocab)\n",
    "        post[\"text\"] = BOW_freq(preproccess(post[\"text\"]), text_vocab)\n",
    "\n",
    "        return {'post': post, 'score': score}\n",
    "\n",
    "class URLTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "\n",
    "        post[\"url\"] = extract_domain(post[\"url\"])\n",
    "\n",
    "        return {'post': post, 'score': score}\n",
    "\n",
    "class TensorTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "\n",
    "        title_list = post[\"title\"]\n",
    "        text_list = post[\"text\"]\n",
    "        time = post[\"time\"]\n",
    "        \n",
    "        output = title_list + text_list\n",
    "        output.append(time)\n",
    "\n",
    "        output = torch.FloatTensor(output)\n",
    "        score = torch.FloatTensor(score)\n",
    "\n",
    "        return {\"post\": output, \"score\": score}\n",
    "        \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HackerNewsPostDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, labels, cutoff = None, transforms = None):\n",
    "        self.posts = data\n",
    "        self.scores = labels\n",
    "        self.transforms = transforms\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        post = self.posts.loc[index]\n",
    "        score = self.scores[index]\n",
    "\n",
    "        if self.cutoff:\n",
    "            score = [(1 if isinstance(score, float) and score > self.cutoff else 0)]\n",
    "\n",
    "        sample = {'post': post, 'score': score}\n",
    "\n",
    "        if self.transforms:\n",
    "            for transform in self.transforms:\n",
    "                sample = transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_to_index = {word: index for index, word in enumerate(titles_vocab)}\n",
    "text_to_index = {word: index for index, word in enumerate(text_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, lables, cutoff):\n",
    "        self.posts = data\n",
    "        self.scores = lables\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        post = self.posts.loc[index]\n",
    "        score = self.scores[index]\n",
    "\n",
    "        title_indexs = preproccess(post[\"title\"])\n",
    "        text_indexs = preproccess(post[\"text\"])\n",
    "\n",
    "        if title_indexs == []:\n",
    "            title_indexs = [\"<|empty|>\"]\n",
    "\n",
    "        if text_indexs == []:\n",
    "            text_indexs = [\"<|empty|>\"]\n",
    "\n",
    "        time = post[\"time\"]\n",
    "\n",
    "        score = 1 if (isinstance(score, float) and score > self.cutoff) else 0\n",
    "\n",
    "        #score = torch.FloatTensor(score)\n",
    "\n",
    "        sample = ((title_indexs, text_indexs, time), score)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non url training dataset\n",
    "transforms = [TextualTransform1(), TensorTransform()]\n",
    "cutoff = 20\n",
    "\n",
    "post_training_dataset = HackerNewsPostDataset(training_data_indexed, scores, cutoff, transforms)\n",
    "post_testing_dataset = HackerNewsPostDataset(testing_data_indexed, testing_scores, cutoff, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dataset_train = EmbeddingsDataset(training_data_indexed, scores, cutoff)\n",
    "embedding_dataset_test = EmbeddingsDataset(testing_data_indexed, testing_scores, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_embed(batch):\n",
    "    lables, texts, offsets = [], [], [0]\n",
    "    for post, score in batch:\n",
    "\n",
    "        titles_indexs = [titles_to_index[word] for word in post[0]]\n",
    "        text_indexs = [text_to_index[word] for word in post[1]]\n",
    "        \n",
    "        proccessed_input = torch.tensor(titles_indexs + text_indexs, dtype = torch.int64)\n",
    "\n",
    "        texts.append(proccessed_input)\n",
    "        lables.append([score])\n",
    "        offsets.append(proccessed_input.size(0))\n",
    "\n",
    "    lables = torch.tensor(lables, dtype = torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim = 0)\n",
    "    texts = torch.cat(texts)\n",
    "    return lables, texts, offsets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(post_testing_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1960 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "batch_size = 100\n",
    "num_iterations = 9800\n",
    "num_epochs = 5\n",
    "train_loader = torch.utils.data.DataLoader(dataset=post_training_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=post_testing_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_embed = torch.utils.data.DataLoader(dataset = embedding_dataset_train, batch_size=batch_size, shuffle=True, \n",
    "                                                collate_fn = collate_batch_embed)\n",
    "test_loader_embed = torch.utils.data.DataLoader(dataset = embedding_dataset_test, batch_size=batch_size, shuffle=True, \n",
    "                                                collate_fn = collate_batch_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader_embed)\n",
    "first = next(it)\n",
    "print(len(first[0]), first[0])\n",
    "print(len(first[1]))\n",
    "print(len(first[2]), first[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic Feed Forward Neural Network\n",
    "\n",
    "class FFNetwork(nn.Module):\n",
    "    def __init__(self, input_dimensions, hidden_dimensions, output_dimensions):\n",
    "        super(FFNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dimensions, hidden_dimensions)\n",
    "\n",
    "        self.nonlinear = nn.ReLU()\n",
    "    \n",
    "        self.linear2 = nn.Linear(hidden_dimensions, output_dimensions)\n",
    "\n",
    "        self.sigmoid = torch.sigmoid\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        x = self.nonlinear(x)\n",
    "\n",
    "        output = self.linear2(x)\n",
    "\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic Feed Forward Neural Network (Regression)\n",
    "\n",
    "class FFNetworkReg(nn.Module):\n",
    "    def __init__(self, input_dimensions, hidden_dimensions, output_dimensions):\n",
    "        super(FFNetworkReg, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dimensions, hidden_dimensions)\n",
    "\n",
    "        self.nonlinear = nn.ReLU()\n",
    "    \n",
    "        self.linear2 = nn.Linear(hidden_dimensions, output_dimensions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        x = self.nonlinear(x)\n",
    "\n",
    "        output = self.linear2(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNetworkEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, output_dim):\n",
    "        super(FFNetworkEmbedding, self).__init__()\n",
    "\n",
    "        self.embed = nn.EmbeddingBag(input_dim, embedding_dim)\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim, 2056)\n",
    "        self.linear2 = nn.Linear(2056, 256)\n",
    "        self.linear3 = nn.Linear(256, 64)\n",
    "        self.linear4 = nn.Linear(64, 16)\n",
    "        self.linear5 = nn.Linear(16, output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, offsets):\n",
    "\n",
    "        # apply an emedding bag layer to get average of all embeddings\n",
    "        x = self.embed(x, offsets)\n",
    "\n",
    "        # apply linear functions\n",
    "        x = self.relu(self.linear1(x))\n",
    "\n",
    "        x = self.relu(self.linear2(x))\n",
    "\n",
    "        x = self.relu(self.linear3(x))\n",
    "\n",
    "        x = self.relu(self.linear4(x))\n",
    "\n",
    "        x = self.relu(self.linear5(x))\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dimensions of the basic model\n",
    "input_dimensions = len(text_vocab) + len(titles_vocab) + 1\n",
    "hidden_dimensions = 1000\n",
    "output_dimensions = 1\n",
    "\n",
    "# instantiate the class we are using for this model\n",
    "model = FFNetwork(input_dimensions, hidden_dimensions, output_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dimensions of the basic model\n",
    "input_dimensions_reg = 100\n",
    "hidden_dimensions_reg = 1000\n",
    "output_dimensions_reg = 1\n",
    "\n",
    "# instantiate the class we are using for this model\n",
    "model_reg = FFNetworkReg(input_dimensions_reg, hidden_dimensions_reg, output_dimensions_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_embed = max(len(text_vocab), len(titles_vocab))\n",
    "embedding_dim = 256\n",
    "output_dimensions_embed = 1\n",
    "\n",
    "model_embed = FFNetworkEmbedding(input_dim_embed, embedding_dim, output_dimensions_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it = iter(train_loader)\n",
    "#print(model(torch.stack(next(it)[\"post\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions class\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer class\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for (batch_index, batch) in enumerate(train_loader):\n",
    "#    if(batch_index > 0):\n",
    "#        break\n",
    "#    print(model(batch[\"post\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for (batch_index, batch) in enumerate(train_loader):\n",
    "#    if(batch_index > 0):\n",
    "#        break\n",
    "#    print(model_reg(batch[\"post\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for calculating the accuracy of the model\n",
    "def get_model_accuracy(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for (batch_index, batch) in enumerate(loader):\n",
    "        \n",
    "        model.float()\n",
    "        posts = batch[\"post\"]\n",
    "        scores = batch[\"score\"]\n",
    "\n",
    "        # get predirction probablities\n",
    "        predictions_prob = model(posts)\n",
    "\n",
    "        # get class predictions\n",
    "        _, predictied = torch.max(predictions_prob.data, 1)\n",
    "\n",
    "        # calculate tota samples predicted and correct\n",
    "        total = total + scores.size(0)\n",
    "        correct = correct + (predictied == scores).sum()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for training model\n",
    "def train_model(model, train_loader, test_loader, loss, optimizer):\n",
    "    iteration = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Starting Epoch: \" + str(epoch))\n",
    "        for (batch_index, batch) in enumerate(train_loader):\n",
    "            #print(\"Iteration \" + str(iteration))\n",
    "            model.float()\n",
    "            posts = batch[\"post\"]\n",
    "            scores = batch[\"score\"]\n",
    "\n",
    "            # set grads to 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(posts)\n",
    "\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_func(predictions, scores)\n",
    "\n",
    "            # backwards pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            if iteration % 50 == 0:\n",
    "                print(\"\\n\")\n",
    "                accuracy = get_model_accuracy(model, test_loader)\n",
    "                print(\"Iteration {}. Loss {}. Accuracy {}\".format(iteration, loss.item(), accuracy))\n",
    "                print(\"\\n\")\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_embed(model, train_loader, test_loader, loss, optimizer):\n",
    "    iterations = 0\n",
    "    accuracy_list = []\n",
    "    loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for index, batch in enumerate(train_loader):\n",
    "            lables, texts, offsets = batch\n",
    "\n",
    "            model.train(True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(texts, offsets)\n",
    "\n",
    "            loss = loss_func(predictions, lables.float())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if iterations % 100 == 0:\n",
    "                total = 0\n",
    "                correct = 0\n",
    "\n",
    "                for test_index, test_batch in enumerate(test_loader):\n",
    "\n",
    "                    test_lables, test_texts, test_offsets = test_batch\n",
    "\n",
    "                    output_preds = model(test_texts, test_offsets)\n",
    "                    output_preds = [1 if output > 0.0 else 0 for output in output_preds]\n",
    "\n",
    "                    total = total + test_lables.size(0)\n",
    "\n",
    "                    for i in range(len(test_lables)):\n",
    "                        if output_preds[i] == test_lables[i]:\n",
    "                            correct += 1\n",
    "\n",
    "                accuracy = correct / total\n",
    "                print(\"Iteration {}. Loss {}. Accuracy {}\".format(iterations, loss.item(), accuracy))\n",
    "                accuracy_list.append(accuracy)\n",
    "                loss_list.append(loss.item())\n",
    "        iterations = iterations + 1\n",
    "    return model, accuracy_list, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed_final, accs, losses = train_model_embed(model_embed, train_loader_embed, test_loader_embed, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = train_model(model, train_loader, test_loader, loss_func, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
