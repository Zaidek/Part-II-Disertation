{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ML libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "with open(\"../data/training_data\", \"rb\") as fb:\n",
    "    training_data = pickle.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import testing data\n",
    "with open(\"../data/testing_data\", \"rb\") as fb:\n",
    "    testing_data = pickle.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try set gpu as training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "training_data = training_data.loc[training_data.type == \"story\"]\n",
    "testing_data = testing_data.loc[testing_data.type == \"story\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_indexed = training_data.reset_index(drop=True)\n",
    "testing_data_indexed = testing_data.reset_index(drop=True)\n",
    "print(training_data_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preproccess(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    text = re.sub(\"\\d-\", \"\", text)\n",
    "    lower = text.lower()\n",
    "    tokens = re.split('\\s+', lower)\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    final = [word for word in tokens if word not in stop_words]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data_indexed[\"titles_processed\"] = training_data_indexed[\"title\"].apply(lambda x: preproccess(x))\n",
    "#training_data_indexed[\"text_processed\"] = training_data_indexed[\"text\"].apply(lambda x: preproccess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titles_vocab = list(dict.fromkeys(sum(list(training_data_indexed.titles_processed), [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_vocab = list(dict.fromkeys(sum(list(training_data_indexed.text_processed), [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"titles_vocab\", \"rb\") as fb:\n",
    "    titles_vocab = pickle.load(fb)\n",
    "\n",
    "with open(\"text_vocab\", \"rb\") as fb:\n",
    "    text_vocab = pickle.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data_indexed.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_cols_with_url = [\"title\", \"url\", \"text\", \"time\"]\n",
    "req_cols_without_url = [\"title\", \"text\", \"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = training_data_indexed.score\n",
    "training_data_indexed = training_data_indexed[req_cols_without_url]\n",
    "\n",
    "testing_scores = testing_data_indexed.score\n",
    "testing_data_indexed = testing_data_indexed[req_cols_without_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data_indexed.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW_bin(words, vocab):\n",
    "    return [1 if word in words else 0 for word in vocab]\n",
    "\n",
    "def BOW_freq(words, vocab):\n",
    "    return [words.count(word) for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(url):\n",
    "    if not isinstance(url, str):\n",
    "        return \"\"\n",
    "    return urlparse(url).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformations of the data\n",
    "\n",
    "class TextualTransform1(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "        \n",
    "        post[\"title\"] = BOW_bin(preproccess(post[\"title\"]), titles_vocab)\n",
    "        post[\"text\"] = BOW_bin(preproccess(post[\"title\"]), text_vocab)\n",
    "\n",
    "        return {'post': post, 'score': score}\n",
    "\n",
    "class TextualTransform2(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "        \n",
    "        post[\"title\"] = BOW_freq(preproccess(post[\"title\"]), titles_vocab)\n",
    "        post[\"text\"] = BOW_freq(preproccess(post[\"title\"]), text_vocab)\n",
    "\n",
    "        return {'post': post, 'score': score}\n",
    "\n",
    "class URLTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "\n",
    "        post[\"url\"] = extract_domain(post[\"url\"])\n",
    "\n",
    "        return {'post': post, 'score': score}\n",
    "\n",
    "class TensorTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "\n",
    "        title_list = post[\"title\"]\n",
    "        text_list = post[\"text\"]\n",
    "        time = post[\"time\"]\n",
    "\n",
    "        output = title_list + text_list\n",
    "        output.append(time)\n",
    "        output = torch.FloatTensor(output)\n",
    "        output = [out.float() for out in output]\n",
    "\n",
    "        return {\"post\": output, \"score\": score}\n",
    "        \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HackerNewsPostDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, labels, transforms = None):\n",
    "        self.posts = data\n",
    "        self.scores = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        post = self.posts.loc[index]\n",
    "        score = self.scores[index]\n",
    "\n",
    "        sample = {'post': post, 'score': score}\n",
    "\n",
    "        if self.transforms:\n",
    "            for transform in self.transforms:\n",
    "                sample = transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non url training dataset\n",
    "transforms = [TextualTransform1(), TensorTransform()]\n",
    "\n",
    "post_training_dataset = HackerNewsPostDataset(training_data_indexed, scores, transforms)\n",
    "post_testing_dataset = HackerNewsPostDataset(testing_data_indexed, testing_scores, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1960 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "batch_size = 100\n",
    "num_iterations = 9800\n",
    "num_epochs = 5\n",
    "train_loader = torch.utils.data.DataLoader(dataset=post_training_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=post_testing_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic Feed Forward Neural Network\n",
    "\n",
    "class FFNetwork(nn.Module):\n",
    "    def __init__(self, input_dimensions, hidden_dimensions, output_dimensions):\n",
    "        super(FFNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dimensions, hidden_dimensions)\n",
    "\n",
    "        self.nonlinear = nn.ReLU()\n",
    "    \n",
    "        self.linear2 = nn.Linear(hidden_dimensions, output_dimensions)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x2 = self.linear1(x)\n",
    "\n",
    "        x3 = self.nonlinear(x2)\n",
    "\n",
    "        output = self.linear2(x3)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic Feed Forward Neural Network (Regression)\n",
    "\n",
    "class FFNetworkReg(nn.Module):\n",
    "    def __init__(self, input_dimensions, hidden_dimensions, output_dimensions):\n",
    "        super(FFNetworkReg, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dimensions, hidden_dimensions)\n",
    "\n",
    "        self.nonlinear = nn.ReLU()\n",
    "    \n",
    "        self.linear2 = nn.Linear(hidden_dimensions, output_dimensions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x2 = self.linear1(x)\n",
    "\n",
    "        x3 = self.nonlinear(x2)\n",
    "\n",
    "        output = self.linear2(x3)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dimensions of the basic model\n",
    "input_dimensions = len(titles_vocab) + len(text_vocab) + 1 \n",
    "hidden_dimensions = 1000\n",
    "output_dimensions = 2\n",
    "\n",
    "# instantiate the class we are using for this model\n",
    "model = FFNetwork(input_dimensions, hidden_dimensions, output_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dimensions of the basic model\n",
    "input_dimensions_reg = len(titles_vocab) + len(text_vocab) + 1 \n",
    "hidden_dimensions_reg = 1000\n",
    "output_dimensions_reg = 1\n",
    "\n",
    "# instantiate the class we are using for this model\n",
    "model_reg = FFNetworkReg(input_dimensions_reg, hidden_dimensions_reg, output_dimensions_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(model_reg(batch[\"posts\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions class\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer class\n",
    "learning_rate = 0.2\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for (batch_index, batch) in enumerate(train_loader):\n",
    "#    if(batch_index > 0):\n",
    "#        break\n",
    "#    print(model(batch[\"post\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for (batch_index, batch) in enumerate(train_loader):\n",
    "#    if(batch_index > 0):\n",
    "#        break\n",
    "#    print(model_reg(batch[\"post\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for calculating the accuracy of the model\n",
    "def get_model_accuracy(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for (batch_index, batch) in enumerate(loader):\n",
    "        \n",
    "        posts = batch[\"post\"]\n",
    "        scores = batch[\"score\"]\n",
    "\n",
    "        # get predirction probablities\n",
    "        predictions_prob = model(posts)\n",
    "\n",
    "        # get class predictions\n",
    "        _, predictied = torch.max(predictions_prob.data, 1)\n",
    "\n",
    "        # calculate tota samples predicted and correct\n",
    "        total = total + scores.size(0)\n",
    "        correct = correct + (predictied == scores).sum()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for training model\n",
    "def train_model(model, train_loader, test_loader, loss, optimizer):\n",
    "    iteration = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Starting Epoch: \" + str(epoch))\n",
    "        for (batch_index, batch) in enumerate(train_loader):\n",
    "            print(\"Iteration \" + str(iteration))\n",
    "\n",
    "            posts = batch[\"post\"]\n",
    "            scores = batch[\"score\"]\n",
    "\n",
    "            # set grads to 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            predictions = model(posts)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_func(predictions, scores)\n",
    "\n",
    "            # backwards pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            if iteration % 10 == 0:\n",
    "                print(\"\\n\")\n",
    "                accuracy = get_model_accuracy(model, test_loader)\n",
    "                print(\"Iteration {}. Loss {}. Accuracy {}\".format(iteration, loss.item(), accuracy))\n",
    "                print(\"\\n\")\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = train_model(model_reg, train_loader, test_loader, loss_func, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 26 2021, 20:14:08) \n[GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
