{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ML libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import openai\n",
    "\n",
    "# As dataset is imbalanced define a weighted sampler\n",
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preproccessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preproccess(text):\n",
    "    if isinstance(text, float):\n",
    "        return [\"\"]\n",
    "    \n",
    "    # split into tokens\n",
    "    tokens = re.split('\\s+', text)\n",
    "\n",
    "    # remove punctuation\n",
    "    tokens = [\"\".join([i for i in x if i not in string.punctuation]) for x in tokens]\n",
    "\n",
    "    # remove numbers\n",
    "    tokens = [re.sub(\"\\d+\", \"\", x) for x in tokens]\n",
    "\n",
    "    # make all tokens lowercase\n",
    "    tokens = [x.lower() for x in tokens]\n",
    "\n",
    "    # remove tokens which are too short or too long\n",
    "    tokens = [token for token in tokens if len(token) > 2 and len(token) < 15]\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tokens = [token for token in tokens if not (token.startswith(\"http\") or token.startswith(\"www\") or token.endswith(\"com\"))]\n",
    "\n",
    "    # remove stop words\n",
    "    #final = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    if isinstance(tokens, float):\n",
    "        return [\"\"]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basic Bag of Words function\n",
    "def BOW_bin(words, vocab):\n",
    "    return [1 if word in words else 0 for word in vocab]\n",
    "\n",
    "def BOW_freq(words, vocab):\n",
    "    return [words.count(word) for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting domain name form url\n",
    "def extract_domain(url):\n",
    "    if not isinstance(url, str):\n",
    "        return \"\"\n",
    "    return urlparse(url).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for gettings the embeddings of text\n",
    "def get_embeddings(text, model = \"text-similarity-ada-001\"):\n",
    "    embeddings = openai.Embedding.create(input = text, model = model)\n",
    "    return embeddings[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformations of the data\n",
    "class TextualTransform1(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "        \n",
    "        post[\"title\"] = BOW_bin(preproccess(post[\"title\"]), titles_vocab)\n",
    "        post[\"text\"] = BOW_bin(preproccess(post[\"text\"]), text_vocab)\n",
    "\n",
    "        return {'post': post, 'score': score}\n",
    "\n",
    "class TextualTransform2(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "        \n",
    "        post[\"title\"] = BOW_freq(preproccess(post[\"title\"]), titles_vocab)\n",
    "        post[\"text\"] = BOW_freq(preproccess(post[\"text\"]), text_vocab)\n",
    "\n",
    "        return {'post': post, 'score': score}\n",
    "\n",
    "class URLTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "\n",
    "        post[\"url\"] = extract_domain(post[\"url\"])\n",
    "\n",
    "        return {'post': post, 'score': score}\n",
    "\n",
    "class TensorTransform(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        post, score = sample[\"post\"], sample[\"score\"]\n",
    "\n",
    "        title_list = post[\"title\"]\n",
    "        text_list = post[\"text\"]\n",
    "        time = post[\"time\"]\n",
    "        \n",
    "        output = title_list + text_list\n",
    "        output.append(time)\n",
    "\n",
    "        output = torch.FloatTensor(output)\n",
    "        score = torch.FloatTensor(score)\n",
    "\n",
    "        return {\"post\": output, \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a index dict for getting word embeddings\n",
    "titles_to_index = {word: index for index, word in enumerate(titles_vocab)}\n",
    "text_to_index = {word: index + len(titles_to_index) for index, word in enumerate(text_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "from ipynb.fs.defs.datasets import HackerNewsPostDataset\n",
    "from ipynb.fs.defs.datasets import EmbeddingsDataset\n",
    "from ipynb.fs.defs.datasets import BertTitleEmbeddingDataset\n",
    "from ipynb.fs.defs.datasets import BertProcessedTitleEmbeddingDataset\n",
    "from ipynb.fs.defs.datasets import BertProcessedTitleEmbeddingTitleAndTextDataset\n",
    "from ipynb.fs.defs.datasets import BertProcessedTitleEmbeddingTitleAndTimeDataset\n",
    "from ipynb.fs.defs.datasets import BertProcessedTitleEmbeddingMulticlass\n",
    "from ipynb.fs.defs.datasets import OpenAIEmbeddingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non url training dataset for BoW model\n",
    "transforms = [TextualTransform1(), TensorTransform()]\n",
    "\n",
    "post_training_dataset = HackerNewsPostDataset(training_data_indexed, scores, cutoff, transforms)\n",
    "post_validation_dataset = HackerNewsPostDataset(validation_data_indexed, validation_scores, cutoff, transforms)\n",
    "post_testing_dataset = HackerNewsPostDataset(testing_data_indexed, testing_scores, cutoff, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sampler(sample_scores, cutoff):\n",
    "    num_popular = (np.array(sample_scores) > cutoff).sum()\n",
    "    unpopular_weight = 1 / ((len(sample_scores) - num_popular) / num_popular)\n",
    "    popular_weight = 1\n",
    "\n",
    "\n",
    "    class_weights = [unpopular_weight, popular_weight]\n",
    "    weights = [0] * len(sample_scores)\n",
    "\n",
    "    for index, score in enumerate(sample_scores):\n",
    "        weight = class_weights[int(score > cutoff)]\n",
    "        weights[index] = weight\n",
    "\n",
    "    sampler = WeightedRandomSampler(weights, num_samples = len(weights), replacement = True)\n",
    "\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for collating batches\n",
    "def collate_batch_embed(batch):\n",
    "    lables, texts, offsets = [], [], [0]\n",
    "    for post, score in batch:\n",
    "\n",
    "        titles_indexs = [titles_to_index[word] for word in post[0]]\n",
    "        text_indexs = [text_to_index[word] for word in post[1]]\n",
    "        \n",
    "        proccessed_input = torch.tensor(titles_indexs + text_indexs, dtype = torch.int64).to(device)\n",
    "\n",
    "        texts.append(proccessed_input)\n",
    "        lables.append([score])\n",
    "        offsets.append(proccessed_input.size(0))\n",
    "\n",
    "    lables = torch.tensor(lables, dtype = torch.int64).to(device)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim = 0).to(device)\n",
    "    texts = torch.cat(texts)\n",
    "    return lables, texts, offsets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights_and_gradients(model):\n",
    "    for param in model.parameters():\n",
    "        print(f\"Gradient = {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for training a embedding model\n",
    "def train_model_embed_bag(model, train_loader, test_loader, loss_func, optimizer, device, num_epochs):\n",
    "    iterations = 1\n",
    "    accuracy_list = []\n",
    "    loss_list = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    # make sure model is being trained on gpu\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}.\".format(epoch))\n",
    "        running_loss = 0\n",
    "\n",
    "        for index, batch in enumerate(train_loader):\n",
    "            lables, texts, offsets = batch\n",
    "\n",
    "            # set the model to track graidents for training\n",
    "            model.train(True)\n",
    "\n",
    "            # reset optimizer grads to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass of the model\n",
    "            predictions, embeddings = model(texts, offsets)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_func(predictions, lables.float())\n",
    "\n",
    "            # perform backwards pass\n",
    "            loss.backward()\n",
    "\n",
    "            # tune weights with optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # stop gradient tracking for writing current model metrics\n",
    "            model.train(False)          \n",
    "\n",
    "            # track the running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "            # add loss and embeddings to lists\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            embeddings_list.append(embeddings)\n",
    "\n",
    "            if iterations % 100 == 0:\n",
    "\n",
    "                print(\" Iteration {}. Running Loss {}.\".format( iterations, running_loss / 1000))\n",
    "    \n",
    "                # reset running loss\n",
    "                running_loss = 0\n",
    "\n",
    "                \"\"\"\n",
    "                for test_index, test_batch in enumerate(test_loader):\n",
    "\n",
    "                    test_lables, test_texts, test_offsets = test_batch\n",
    "\n",
    "                    output_preds = model(test_texts, test_offsets)\n",
    "                    output_preds = [1 if output > 0.0 else 0 for output in output_preds]\n",
    "\n",
    "                    total = total + test_lables.size(0)\n",
    "\n",
    "                    for i in range(len(test_lables)):\n",
    "                        if output_preds[i] == test_lables[i]:\n",
    "                            correct += 1\n",
    "                \n",
    "\n",
    "                accuracy = correct / total\n",
    "                print(\"Iteration {}. Loss {}. Accuracy {}\".format(iterations, loss.item(), accuracy))\n",
    "                accuracy_list.append(accuracy)\n",
    "                loss_list.append(loss.item())\n",
    "                \"\"\"\n",
    "            iterations = iterations + 1\n",
    "    return model, embeddings_list, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for training a embedding model\n",
    "def train_model_bert(model, train_loader, valid_loader, loss_func, optimizer, device, num_epochs, model_path):\n",
    "    min_vloss = np.inf\n",
    "    #min_loss = np.inf\n",
    "    loss_list = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # make sure model is being trained on gpu\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}.\".format(epoch))\n",
    "        running_loss = 0\n",
    "        running_f1 = 0\n",
    "        iterations = 1\n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "\n",
    "            (embeddings, score) = batch\n",
    "\n",
    "            embeddings = embeddings.to(device)\n",
    "            score = score.to(device)\n",
    "\n",
    "            # set the model to track graidents for training\n",
    "            model.train(True)\n",
    "\n",
    "            # reset optimizer grads to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass of the model\n",
    "            predictions = model(embeddings)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_func(predictions, score.float().unsqueeze(1))\n",
    "\n",
    "            # perform backwards pass\n",
    "            loss.backward()\n",
    "\n",
    "            # tune weights with optimizer\n",
    "            optimizer.step()     \n",
    "\n",
    "            # track the running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # track the running f1-score\n",
    "            running_f1 += f1_score(score, [1 if x > 0.5 else 0 for x in nn.Sigmoid()(predictions).detach().numpy()], zero_division=1)\n",
    "\n",
    "            if iterations % 100 == 0:\n",
    "\n",
    "                print(\"Iteration {}. Current Min Loss {}. Running F1 {}.\".format(iterations, running_loss / 100, running_f1 / 100))\n",
    "\n",
    "                # add running loss = list\n",
    "                loss_list.append(running_loss / 100)\n",
    "\n",
    "                # reset running loss\n",
    "                running_loss = 0\n",
    "\n",
    "                # reset running f1-score\n",
    "                running_f1 = 0\n",
    "\n",
    "                # print current gradients\n",
    "                #print_weights_and_gradients(model)\n",
    "\n",
    "               \n",
    "            iterations = iterations + 1\n",
    "        \n",
    "        # stop gradient tracking for writing current model metrics\n",
    "        model.eval()\n",
    "\n",
    "        # run current model on test dataset to find best model\n",
    "        valid_loss_total = 0\n",
    "        for test_index, test_batch in enumerate(valid_loader):\n",
    "\n",
    "            (valid_e, valid_s) = test_batch\n",
    "\n",
    "            valid_e = valid_e.to(device)\n",
    "            valid_s = valid_s.to(device)\n",
    "\n",
    "            valid_pred = model(valid_e)\n",
    "\n",
    "            valid_loss = loss_func(valid_pred, valid_s.float().unsqueeze(1))\n",
    "            valid_loss_total += valid_loss.item()\n",
    "\n",
    "        valid_losses.append(valid_loss_total / len(valid_loader))\n",
    "        print(\"vloss -\", valid_loss_total)\n",
    "\n",
    "        if valid_loss_total < min_vloss:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            min_vloss = valid_loss_total\n",
    "\n",
    "    return model, loss_list, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for training a embedding model\n",
    "def train_model_bert_multi(model, train_loader, valid_loader, loss_func, optimizer, device, num_epochs):\n",
    "    min_vloss = np.inf\n",
    "    loss_list = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # make sure model is being trained on gpu\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}.\".format(epoch))\n",
    "        running_loss = 0\n",
    "        running_f1 = 0\n",
    "        iterations = 1\n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "\n",
    "            (embeddings, score) = batch\n",
    "\n",
    "            embeddings = embeddings.to(device)\n",
    "            \n",
    "            score = torch.tensor([x.numpy() for x in score], dtype=torch.float32).transpose(0, 1)\n",
    "\n",
    "            score = score.to(device)\n",
    "\n",
    "            # set the model to track graidents for training\n",
    "            model.train(True)\n",
    "\n",
    "            # reset optimizer grads to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass of the model\n",
    "            predictions = model(embeddings)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_func(predictions, score)\n",
    "\n",
    "            # perform backwards pass\n",
    "            loss.backward()\n",
    "\n",
    "            # tune weights with optimizer\n",
    "            optimizer.step()     \n",
    "\n",
    "            # track the running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # track the running f1-score\n",
    "            running_f1 += f1_score(score, [1 if x > 0.5 else 0 for x in nn.Sigmoid()(predictions).detatch.numpy()])\n",
    "\n",
    "            if iterations % 10 == 0:\n",
    "\n",
    "                print(\"Iteration {}. Running Loss {}. Running F1 {}\".format(iterations, running_loss / 10, running_f1 / 10))\n",
    "\n",
    "                # add running loss = list\n",
    "                loss_list.append(running_loss / 10)\n",
    "\n",
    "                # reset running loss\n",
    "                running_loss = 0\n",
    "               \n",
    "            iterations = iterations + 1\n",
    "        \n",
    "        # stop gradient tracking for writing current model metrics\n",
    "        model.eval()\n",
    "\n",
    "        # run current model on test dataset to find best model\n",
    "        valid_loss_total = 0\n",
    "        for test_index, test_batch in enumerate(valid_loader):\n",
    "\n",
    "            (valid_e, valid_s) = test_batch\n",
    "\n",
    "            valid_e = valid_e.to(device)\n",
    "            valid_s = valid_s.to(device)\n",
    "\n",
    "            valid_pred = model(valid_e)\n",
    "\n",
    "            valid_loss = loss_func(valid_pred, valid_s.float().unsqueeze(1))\n",
    "            valid_loss_total += valid_loss.item()\n",
    "\n",
    "        valid_losses.append(valid_loss_total / len(valid_loader))\n",
    "        print(\"vloss -\", valid_loss_total)\n",
    "\n",
    "        if valid_loss_total < min_vloss:\n",
    "            torch.save(model.state_dict(), \"bert_model_best_3.pth\")\n",
    "            min_vloss = valid_loss_total\n",
    "\n",
    "    return model, loss_list, valid_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8cd98ac651c668ce2c6203d75b23f2d5bc0a45f06efaf825f1ea3a340dc3a78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
