{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ML libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import embeddings modules\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from pytorch_transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "with open(\"../data/training_data\", \"rb\") as fb:\n",
    "    training_data = pickle.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get non-comments and reset index\n",
    "training_data = training_data.loc[training_data.type == \"story\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevent columns and remove nan values\n",
    "training_data = training_data[[\"title\", \"text\", \"url\", \"score\"]]\n",
    "training_data.title = training_data.title.fillna(\"\")\n",
    "training_data.text = training_data.text.fillna(\"\")\n",
    "training_data.url = training_data.url.fillna(\"\")\n",
    "training_data.score = training_data.score.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bert objects\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bert class\n",
    "class BertEmbeddings(object):\n",
    "    \n",
    "    # initally define class with a model and a tokenizer to be used\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # setter for model\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    # setter for tokenizer\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # convert text into vert tokens\n",
    "    def tokenize_text(self, text):\n",
    "        # add special start and end tokens\n",
    "        text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "        # tokenize sentence\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        # get vocab indicies\n",
    "        tokens_index = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # convert into pytorch tensors\n",
    "        output = torch.tensor([tokens_index])\n",
    "\n",
    "        return output\n",
    "        \n",
    "    # apply bert model to get embeddings for single input\n",
    "    def apply_model(self, tokens):\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "           output = self.model(tokens)\n",
    "        return output[2]\n",
    "\n",
    "    # reshape the output of the model\n",
    "    def reshape_token_embeddings(self, token_embeddings):\n",
    "\n",
    "        # combine all of the layers of the model\n",
    "        token_embeddings = torch.stack(token_embeddings, dim=0)\n",
    "\n",
    "        # get rid of the batch layer as we only use 1 sentence per input\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "        # change order of layers and tokens\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "        # return transformed embeddings\n",
    "        return token_embeddings\n",
    "    \n",
    "\n",
    "    # sum the \n",
    "    def get_word_embedding_concat(self, token_embeddings, index):\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def get_word_embedding_sum(self, token_embeddings, index):\n",
    "        \n",
    "        return 0\n",
    "\n",
    "\n",
    "    # define word embedding by averging all the token embeddings from the second to last layer\n",
    "    def get_sentence_embeddings(self, token_embeddings):\n",
    "\n",
    "        token_tensors = token_embeddings[-2]\n",
    "\n",
    "        sentence_embedding = torch.mean(token_tensors, dim=0)\n",
    "\n",
    "        return sentence_embedding\n",
    "\n",
    "    # Given an input sentence return the coresponding sentence embedding\n",
    "    def get_embedding(self, input):\n",
    "\n",
    "        tokens = self.tokenize_text(input)\n",
    "\n",
    "        token_embeddings = self.apply_model(tokens)\n",
    "\n",
    "        token_embeddings = self.reshape_token_embeddings(token_embeddings)\n",
    "\n",
    "        sentence_embedding = self.get_sentence_embeddings(token_embeddings)\n",
    "\n",
    "        return sentence_embedding\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pytorch embedding model class\n",
    "class PytorchEmbeddings(object):\n",
    "\n",
    "    def __init__(self, vocab_path, embedding_dim):\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        with open(vocab_path, \"rb\") as fb:\n",
    "            vocab = pickle.load(fb)\n",
    "\n",
    "        self.vocab_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "        self.embedding = nn.EmbeddingBag(len(self.vocab_to_index), embedding_dim)\n",
    "\n",
    "    def preproccess(self, text):\n",
    "        if isinstance(text, float):\n",
    "            return [\"\"]\n",
    "        \n",
    "        # split into tokens\n",
    "        tokens = re.split('\\s+', text)\n",
    "\n",
    "        # remove punctuation\n",
    "        tokens = [\"\".join([i for i in x if i not in string.punctuation]) for x in tokens]\n",
    "\n",
    "        # remove numbers\n",
    "        tokens = [re.sub(\"\\d+\", \"\", x) for x in tokens]\n",
    "\n",
    "        # make all tokens lowercase\n",
    "        tokens = [x.lower() for x in tokens]\n",
    "\n",
    "        # remove tokens which are too short or too long\n",
    "        tokens = [token for token in tokens if len(token) > 2 and len(token) < 15]\n",
    "\n",
    "        # remove hyperlinks\n",
    "        tokens = [token for token in tokens if not (token.startswith(\"http\") or token.startswith(\"www\") or token.endswith(\"com\"))]\n",
    "\n",
    "        # remove stop words\n",
    "        #final = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        if isinstance(tokens, float):\n",
    "            return [\"\"]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def get_indexs(self, text):\n",
    "\n",
    "        tokens = self.preproccess(text)\n",
    "\n",
    "        indicies = [self.vocab_to_index[token] for token in tokens]\n",
    "\n",
    "        return torch.tensor(indicies, dtype = torch.int64)\n",
    "\n",
    "    def get_nn_embeddings(self, text):\n",
    "\n",
    "        indicies = self.get_indexs(text)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.embedding(indicies, torch.tensor([0]))\n",
    "            \n",
    "        return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a bert loader\n",
    "bert = BertEmbeddings(bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnModel = PytorchEmbeddings(\"titles_vocab\", 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simularity(embeddings1, embeddings2):\n",
    "    return 1 - cosine(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = bert.get_embedding(\"cat\")\n",
    "t2 = bert.get_embedding(\"dog\")\n",
    "print(get_simularity(t1, t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = nnModel.get_nn_embeddings(\"cat\")[0]\n",
    "t4 = nnModel.get_nn_embeddings(\"dog\")[0]\n",
    "print(get_simularity(t3, t4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/data_train\", \"rb\") as fb:\n",
    "    data_train = pickle.load(fb)\n",
    "\n",
    "with open(\"../data/data_valid\", \"rb\") as fb:\n",
    "    data_valid = pickle.load(fb)\n",
    "\n",
    "with open(\"../data/data_test\", \"rb\") as fb:\n",
    "    data_test = pickle.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.title = data_train.title.fillna(\"\")\n",
    "data_valid.title = data_valid.title.fillna(\"\")\n",
    "data_test.title = data_test.title.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    return openai.Embedding.create(input=text, model = \"text-embedding-ada-002\")['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(openai.Embedding.create(input=\"\", model = \"text-embedding-ada-002\")['data'][0]['embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_index(x):\n",
    "    print(ind)\n",
    "    ind += 1\n",
    "    return get_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings = [0] * len(data_train.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, title in enumerate(data_train.title):\n",
    "    if title_embeddings[ind] == 0:\n",
    "        title_embeddings[ind] = get_embedding(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        for ind, title in enumerate(data_train.title):\n",
    "            if title_embeddings[ind] == 0:\n",
    "                print(ind)\n",
    "                title_embeddings[ind] = get_embedding(title)\n",
    "\n",
    "        break\n",
    "    except:\n",
    "        print(\"broke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_title_embeddings = [0] * len(data_valid.title)\n",
    "test_title_embeddings = [0] * len(data_test.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        for ind, title in enumerate(data_valid.title):\n",
    "            if valid_title_embeddings[ind] == 0:\n",
    "                print(ind)\n",
    "                valid_title_embeddings[ind] = get_embedding(title)\n",
    "\n",
    "        break\n",
    "    except:\n",
    "        print(\"broke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        for ind, title in enumerate(data_test.title):\n",
    "            if test_title_embeddings[ind] == 0:\n",
    "                print(ind)\n",
    "                test_title_embeddings[ind] = get_embedding(title)\n",
    "\n",
    "        break\n",
    "    except:\n",
    "        print(\"broke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_embeddings\", \"wb\") as fb:\n",
    "    pickle.dump(title_embeddings, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_embeddings\", \"wb\") as fb:\n",
    "    pickle.dump(valid_title_embeddings, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_embeddings\", \"wb\") as fb:\n",
    "    pickle.dump(test_title_embeddings, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title_embeddings)):\n",
    "    if title_embeddings[i] == 0:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"title_ada_embeddings\"] = data_train.title.apply(lambda x: print_embedding_index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8cd98ac651c668ce2c6203d75b23f2d5bc0a45f06efaf825f1ea3a340dc3a78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
